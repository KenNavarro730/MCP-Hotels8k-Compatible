models_root: '../models' #path where trained models will be saved
dataset_root: '../datasets/'
vocab_length: 708 #This is specific to our data and was found offline
imgs_root: '/home/tun84049/Desktop/images' #path where the coco images will be found
vocab_root: 'resources/vocabs/'
dataset_path: '/shared/data/hotel_50k_images_with_seg/train/'
dataset_name: Hotels8k
number_of_categories_combined: 2
trainer: probabilistic

dataloader:
    batch_size: 16
    num_workers: 8
    test_num_workers: 8
    word_dim: 300
    random_erasing: 0.2
    caption_dropout: 0.1
    caption_start_end_token: True

# model configuration for the image and text models
model:
    name: pcme
    combiner_type: 'mpc'# 'mpc' for MPC, 'pcme_addition' for PCME + addition, 'pcme_mlp' for PCME + MLP
    num_embeds: 1
    embed_size: 512
    cnn_type: resnet50
    wemb_type: glove
    glove_size: 42B
    word_dim: 300
    n_samples_inference: 7

# optimizer configuration
optimizer:
    name: AdamW
    learning_rate: 0.0002
    lr_decay_rate: 0.1
    lr_decay_step: 600
    weight_decay: 0.00005
    resnet_lr_factor: 0.1

# criterion configuration
criterion:
    train:
        name: mpc #'mpc' for MPC, 'pcme' for (PCME + addition, PCME + MLP)
        temperature: 1
        retrieval_loss_weight: 1
        logsigma_l2_loss_weight: 0.001

train:
    num_epochs: 1600
    val_epochs: 1600
    log_step: 200 #batches between tensorboard logs
    epochs_between_checkpoints: 800

